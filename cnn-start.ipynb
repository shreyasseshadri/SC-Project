{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported data.....\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_pos=pd.read_csv('./data/imdb/train/pos.csv')\n",
    "train_neg=pd.read_csv('./data/imdb/train/neg.csv')\n",
    "test_pos=pd.read_csv('./data/imdb/test/pos.csv')\n",
    "test_neg=pd.read_csv('./data/imdb/test/neg.csv')\n",
    "print('imported data.....')\n",
    "\n",
    "def clear_text(text):\n",
    "    return text.replace('\"', '').replace(\"'\", \"\").replace(\"<br />\",\"\").replace(\",\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"-\",\"\").replace(\"/\",\"\").replace(\"!\",\"\")\n",
    "\n",
    "train_pos_sent=train_pos['text'].apply(clear_text).tolist()\n",
    "train_neg_sent=train_neg['text'].apply(clear_text).tolist()\n",
    "test_pos_sent=test_pos['text'].apply(clear_text).tolist()\n",
    "test_neg_sent=test_neg['text'].apply(clear_text).tolist()\n",
    "\n",
    "\n",
    "corpus_train=train_pos_sent+train_neg_sent\n",
    "corpus_test=test_pos_sent+test_neg_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
      "\u001b[K    100% |################################| 1.4MB 2.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from nltk) (1.11.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.3\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def create_corpus(corpus):\n",
    "    corpus_out=[]\n",
    "    sw=stopwords.words('english')\n",
    "    for review in corpus:\n",
    "        review_sents=review.split(\".\")\n",
    "        for sent in review_sents:\n",
    "            temp=sent.split(\" \")\n",
    "            for word in temp:\n",
    "                if word=='' and word==' ' and word in sw:\n",
    "                    del word\n",
    "#             print(sents)\n",
    "#             print('--------------------------------------')\n",
    "        corpus_out.append(temp)\n",
    "    return corpus_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=create_corpus(corpus_test+corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1491)\n"
     ]
    }
   ],
   "source": [
    "# minl=16000\n",
    "# maxl=0\n",
    "# for rev in corpus_train:\n",
    "#     minl=min(minl,len(rev.split(\" \")))\n",
    "#     maxl=max(maxl,len(rev.split(\" \")))\n",
    "# print(minl,maxl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14327, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(corpus, min_count=1,seed=123)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'scoreToo', 'BrestLitovsk', 'Savage', 'Bowery', 'foul', 'four', 'Does', 'Olympics', 'spiders']\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.bin')\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "def create_words(corpus):\n",
    "    max_word=150\n",
    "    words=[]\n",
    "    flag=0\n",
    "    for review in corpus:\n",
    "        review_words=[]\n",
    "        review_sents=review.split(\".\")\n",
    "        count=1\n",
    "        for sent in review_sents:\n",
    "            temp=sent.split(\" \")\n",
    "            for word in temp:\n",
    "                if len(word)!=0 and word!=' ' and count<=max_word:\n",
    "                    review_words.append(word)\n",
    "                    count+=1\n",
    "    #         review_words.extend([word for word in sent.split(\" \") if word not in stopwords.words('english')])\n",
    "        words.append(review_words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words=create_words(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words=create_words(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.4834586 , -0.8623024 , -0.60484344, -1.4705721 , -0.12694284,\n",
       "        2.1140037 ,  0.76141953, -0.22995406,  0.04501231,  0.09383006,\n",
       "        1.4644676 , -0.7337624 , -1.022652  , -0.5702895 ,  0.98213613,\n",
       "        0.19339153, -1.157489  ,  2.2790394 ,  0.35115936,  1.0630728 ,\n",
       "        0.0264265 , -0.5422355 ,  0.2722535 ,  0.17269169, -0.632745  ,\n",
       "       -0.12009603,  1.8192468 , -0.33960378, -0.86357874, -0.36764932,\n",
       "        0.04084243,  0.4385397 , -0.8824437 , -0.11205492, -0.25662112,\n",
       "       -0.38798663,  0.83016354, -0.35403296,  0.2686252 , -1.120308  ,\n",
       "       -0.14301804, -0.78125286, -1.8001832 ,  1.7826413 , -0.48472777,\n",
       "        0.1489331 ,  1.1054084 ,  0.51163435, -0.54393524,  1.0574402 ,\n",
       "        0.26058695,  0.33748442, -0.6892918 ,  0.04279883, -0.33555952,\n",
       "       -0.11199496,  0.16019207,  1.5494055 , -0.36537984, -1.474116  ,\n",
       "        0.66464907,  0.12885463,  0.61771953,  0.80684096, -1.7894149 ,\n",
       "        0.02079906,  0.90201986,  0.04289038,  0.33577064,  1.7144858 ,\n",
       "       -0.65555614,  0.35482767,  0.7515677 ,  0.7621203 , -1.1249683 ,\n",
       "        0.0558667 , -0.65421367, -0.29386044,  1.1414425 ,  0.8678058 ,\n",
       "       -0.30056953, -0.8549495 , -0.00760697,  0.76297027,  1.2007891 ,\n",
       "        0.12310967,  0.7921719 ,  1.6612811 ,  0.79267645,  0.946077  ,\n",
       "        0.4475724 , -0.84711057,  0.15433148, -0.3454648 , -0.1664858 ,\n",
       "       -1.2397912 , -0.75199085,  0.38948527,  0.64817333, -0.29654947],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "new_model = Word2Vec.load('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense,Conv2D ,LSTM,Lambda\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "tf_session = K.get_session()\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_dim(X):\n",
    "    conv_outputs=tf.unstack(X,axis=3)\n",
    "    return tf.concat(conv_outputs,axis=1)\n",
    "\n",
    "def get_shape(input_shape):\n",
    "    assert len(input_shape)==4\n",
    "    return (input_shape[0],input_shape[1]*input_shape[3],input_shape[2])\n",
    "\n",
    "def model(input_shape,hidden_size,dense_layer1,dense_layer2):\n",
    "    X_input=Input(input_shape)\n",
    "    X=Conv2D(32, (5, 5), input_shape=input_shape, activation='relu')(X_input)\n",
    "    X=Lambda(stack_dim,output_shape=get_shape)(X)\n",
    "    X=LSTM(hidden_size,return_sequences=True)(X)\n",
    "    X=Dense(dense_layer1, activation='relu')(X)\n",
    "    X=Dense(dense_layer2, activation='relu')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='sent_classifier')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m=model((300,150,1),128,100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 300, 150, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 296, 146, 32)      832       \n",
      "_________________________________________________________________\n",
      "lambda_5 (Lambda)            (None, 9472, 146)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 9472, 128)         140800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9472, 100)         12900     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9472, 2)           202       \n",
      "=================================================================\n",
      "Total params: 154,734\n",
      "Trainable params: 154,734\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
